# AI赛题

## 赛题一：预训练GPT-2

### 背景

近几年，以ChatGPT为代表的大规模预训练生成式语言Large Language Models, LLMs）迅速崛起，并在自然语言处理（NLP）领域取得了重大突破。GPT-2是OpenAI在2019年推出的模型。GPT-2最小的参数量为124M ，适合初学者进行预训练。

### 任务要求

在这个任务中，我们需要从零开始预训练一个GPT-2模型，使用BookCorpus数据集。任务的目标是熟悉大规模语言模型的训练流程，并优化训练过程中的性能。具体要求：

* 使用transformers库中的默认配置对GPT2进行初始化，不要加载预训练模型，也无需下载GPT2的模型权重
* tokenizer可以从预训练GPT2中加载
* 仅需要使用BookCorpus数据集的前10,000条数据进行预训练
* 一共训练3个epoch，训练总时间越短越好
* 在报告中附上training_loss变化的截图、训练的总时间、电脑的硬件配置和优化加速的方法（如果有），并在附件中附上必要的代码，不要上传大模型权重

PS: 考虑到实际情况，我们会尽量选择参数量小的模型和较小计算量的实验设置，以便于在个人电脑上运行。

### GPT-2

[Hugging Face 链接](https://huggingface.co/openai-community/gpt2)

GPT-2是OpenAI 在2019 年推出的第二代生成式预训练模型。GPT-2与GPT-1架构相同，但是使用了更大的数据集 WebText，大约有 40 GB 的文本数据、800 万个文档，并为模型添加了更多参数（达到15 亿个参数），来提高模型的准确性。

### Book Corpus数据集 

[Hugging Face 链接](https://huggingface.co/datasets/bookcorpus/bookcorpus)

BookCorpus是由多伦多大学的Yukun Zhu等人在2015年提出的一个关于书籍的数据集。该数据集主要是由尚未出版的作者写的免费书籍。该数据集的主要统计结果如下：

| 统计项                 | 统计项（英文）                 | 统计结果  |
| :--------------------- | :----------------------------- | :-------- |
| 书籍数量               | # of books                     | 11038     |
| 语句数量               | # of sentences                 | 74004228  |
| 单词数量               | # of words                     | 984846357 |
| 独立单词数（词汇）     | # of unique words              | 1316420   |
| 平均每个语句的单词数量 | mean # of words per sentence   | 13        |
| 每个语句的单词中位数   | median # of words per sentence | 11        |



## 赛题二：通义千问在MMMLU上的表现

### 背景

大模型的浪潮席卷全球，国产大模型也层出不穷，涌现了文心一言、KIMI、豆包、通义千问等一批大模型。其中，通义千问模型在各类评测数据集取得了优秀的表现。

### 任务要求

我们希望用OpenAI近期发布的[MMMLU](https://huggingface.co/Qwen/Qwen2.5-0.5B)的中文测试集([mmlu_ZH-CN.csv](https://huggingface.co/datasets/openai/MMMLU/blob/main/test/mmlu_ZH-CN.csv))，来评测0.5B的参数量的通义千问2.5模型[Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B)。由于MMMLU中的问题较多，我们只需要选取中文测试中主题为道德场景("moral_scenarios")的前100个问题进行测试。

请将prompt设置为

```python
prompt = f"问题: {Question}\nA) {A}\nB) {B}\nC) {C}\nD) {D}\n请你给出答案（只输出一个字母）: "
```

* 将max_length/max_new_tokens设置为512，其他参数保持默认设置
* 在报告中给出测评的准确度和评测所用的时间
* 评测的总时间越短越好，列出使用的优化加速的方法

另外，Qwen2.5-0.5B的参数量过小，模型表现不好是正常现象。

### MMMLU

MMMLU 是一个广泛认可的人工智能模型所获得的一般知识基准测试。它涵盖了来自57个不同类别的广泛主题，从基础水平的知识到高级专业学科，如法律、物理、历史和计算机科学。

OpenAI使用专业的翻译人员将MMMLU的测试集翻译成了14种语言。依靠人工翻译进行这项评估提高了翻译准确性，特别是在像约鲁巴语这样的低资源语言方面。我们将发布专业的人工翻译版本以及我们用来运行评估的代码。

### Qwen2.5-0.5B

Qwen2.5 是 Qwen 大型语言模型系列的最新版本。对于 Qwen2.5，我们发布了多个基础语言模型和指令调优语言模型，参数规模从 0.5 亿到 72 亿不等。Qwen2.5 相较于 Qwen2 带来了以下改进：

- 显著增加了知识量，并在编码和数学方面大幅提升了能力，这得益于我们在这些领域专门的专家模型。
- 在指令跟随、生成长文本（超过 8K 个标记）、理解结构化数据（例如表格）和生成结构化输出（特别是 JSON 格式）方面有了显著改进。增强了对多样化的系统提示的适应性，提高了角色扮演实现和聊天机器人条件设置的效果。
- 支持长达 128K 个标记的长上下文，并能生成多达 8K 个标记的文本。
- 支持超过 29 种语言，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。

## 其他

### 赛题一训练细节

bookcorput数据集，1.1GB，解压后6.6基本，下载大约花费5分钟

处理bookcorput数据集，“Generating train split”，大概15分钟

A100上单卡预训练三个epoch大概15分钟

### 赛题二训练细节

A100单卡上测评大概5分钟

模型大约1个G，显存占用2.5G

实际测评准确率只有20%，基本都选的A

中文测试集大小4.5MB